from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import pandas as pd
from bs4 import BeautifulSoup

# Set up driver path
PATH = r"/Users/arun/Downloads/chromedriver-mac-arm64/chromedriver"
service = Service(PATH)

options = Options()
# options.add_argument("--headless")  # Uncomment to run in headless mode

# Launch browser
driver = webdriver.Chrome(service=service, options=options)

# Open LinkedIn jobs search page
url = "https://www.linkedin.com/jobs/search/?currentJobId=4233399444&f_E=1&f_TPR=r86400&f_WT=1%2C3&geoId=101436253&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true"
driver.get(url)

# Wait for job cards to load
WebDriverWait(driver, 10).until(
    EC.presence_of_all_elements_located((By.CLASS_NAME, "base-card"))
)

job_data = []
job_links = []

# Get all job card links
jobs = driver.find_elements(By.CLASS_NAME, "base-card")
for job in jobs:
    try:
        link = job.find_element(By.TAG_NAME, "a").get_attribute("href")
        if link and link not in job_links:
            job_links.append(link)
    except:
        continue

# Visit each job link (scraping first 5 for demo)
for link in job_links:
    driver.get(link)
    time.sleep(5)  # Wait for job page to load

    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "top-card-layout__title"))
        )
    except:
        continue

    try:
        title = driver.find_element(By.CLASS_NAME, "top-card-layout__title").text
    except:
        title = "N/A"

    try:
        company = driver.find_element(By.CLASS_NAME, "topcard__org-name-link").text
    except:
        company = "N/A"

    try:
        location = driver.find_element(By.CLASS_NAME, "topcard__flavor--bullet").text
    except:
        location = "N/A"

    try:
        soup = BeautifulSoup(driver.page_source, "html.parser")
        description_div = soup.select_one(".description__text--rich")
        description = description_div.get_text(separator="\n", strip=True) if description_div else "N/A"
    except Exception as e:
        description = f"N/A (Error: {str(e)})"

    job_data.append({
        "Title": title,
        "Company": company,
        "Location": location,
        "Job URL": link,
        "Description": description
    })

# Save to CSV
df = pd.DataFrame(job_data)
df.to_csv("linkedin_detailed_jobs.csv", index=False)
print("Saved to linkedin_detailed_jobs.csv")

input("Press Enter to exit...")
driver.quit()
